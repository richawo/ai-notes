Here are expanded entity-rich summaries for the key papers, now about 50-80 words each and including a link:

Here are the paper summaries updated to include up to 3 comma-delimited topics for each:

| Paper | Original Date | Summary | Topics |
|-|-|-|-|
| [Attention is All You Need (Transformer)](https://arxiv.org/abs/1706.03762) | 12 Jun 2017 | Proposes the Transformer architecture for sequence modeling. Uses self-attention instead of recurrence, allowing modeling longer context. Also employs multi-head attention and FFN blocks. Trains much faster than RNN/CNN models. | architecture, self-attention, sequence modeling |
| [Chain of Density](https://arxiv.org/pdf/2309.04269.pdf) | 8 Sep 2023 | Chain-of-density summarization is a new technique that creates highly condensed yet information-rich summaries from long-form text. It iteratively extracts essential entities from the source text and rewrites the summary to incorporate more entities each time (without losing previous entities), resulting in a "chain" of increasingly dense summaries. | summarisation, prompt-engineering, sequence modeling |
| [Graph of Thoughts](https://arxiv.org/pdf/2308.09687.pdf) | 18 Aug 2023 |  Graph of Thoughts (GoT), a new prompting framework that models LLM reasoning as a graph. GoT enables novel transformations like aggregating thoughts. Experiments show GoT improves quality and reduces costs over prior prompting schemes like Tree of Thoughts (ToT). A new metric called volume indicates GoT thoughts incorporate more context. Overall, graph modelling enhances prompting capabilities. | prompt-engineering, graph-based reasoning, network of thoughts |
| [GPT-4V(ision) System Card](https://cdn.openai.com/papers/GPTV_System_Card.pdf) | 25 Sep 2023 | GPT-4 vision is a multimodal LLM, expanding allowing for image-based inputs.  This system card outlines how OpenAI prepared the vision capabilities of GPT-4 for deployment - including evaluations and derisking mesaures | openai, gpt-4v, llm deployment |
